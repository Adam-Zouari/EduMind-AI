# âœ… Installation Complete!

## ğŸ‰ All Dependencies Installed Successfully

All three virtual environments have been set up with their dependencies:

### âœ… venv_ocr (OCR Service)
- **Status**: INSTALLED
- **Dependencies**: PyMuPDF, paddleocr, pytesseract, whisper, pyannote.audio, torch 2.8.0, and more
- **Purpose**: Runs the OCR microservice on port 8000

### âœ… venv_rag (RAG Service)  
- **Status**: INSTALLED
- **Dependencies**: langchain, chromadb, sentence-transformers, streamlit, and more
- **Purpose**: Runs the RAG microservice on port 8001

### âœ… venv_main (Streamlit App)
- **Status**: INSTALLED  
- **Dependencies**: streamlit, requests (lightweight!)
- **Purpose**: Runs the web interface on port 8501

---

## ğŸš€ How to Run Everything

### Option 1: Automated Startup (Recommended)

Simply run:

```powershell
.\start_all_services.bat
```

This will automatically:
1. Start OCR service in a new window
2. Start RAG service in a new window
3. Check/start Ollama if needed
4. Launch the Streamlit app

### Option 2: Manual Startup (4 Terminals)

**Terminal 1 - OCR Service:**
```powershell
cd C:\Users\ademz\Desktop\9raya\MLOps\Project
venv_ocr\Scripts\activate
uvicorn pipeline.ocr_service:app --port 8000
```

**Terminal 2 - RAG Service:**
```powershell
cd C:\Users\ademz\Desktop\9raya\MLOps\Project
venv_rag\Scripts\activate
uvicorn pipeline.rag_service:app --port 8001
```

**Terminal 3 - Ollama:**
```powershell
ollama serve
```

**Terminal 4 - Streamlit App:**
```powershell
cd C:\Users\ademz\Desktop\9raya\MLOps\Project
venv_main\Scripts\activate
cd pipeline
streamlit run app_microservices.py
```

---

## ğŸŒ Access the Application

Once all services are running:

- **ğŸ“± Streamlit Web App**: http://localhost:8501
- **ğŸ”§ OCR API Docs**: http://localhost:8000/docs
- **ğŸ”§ RAG API Docs**: http://localhost:8001/docs

---

## ğŸ“‹ Usage Instructions

1. **Open your browser** to http://localhost:8501

2. **Connect to services** (sidebar):
   - Click "ğŸš€ Connect to Services"
   - Default URLs should work: `http://localhost:8000` and `http://localhost:8001`

3. **Upload & Process Files**:
   - Go to "Upload & Process" tab
   - Upload PDF, DOCX, images, audio, video, or HTML files
   - Files are automatically processed through OCR and ingested to RAG

4. **Ask Questions**:
   - Go to "Ask Questions" tab
   - Type your question about the uploaded documents
   - Get AI-powered answers with source citations

5. **View History**:
   - Check "Chat History" tab for previous Q&A
   - Check "Processed Files" tab for uploaded documents

---

## ğŸ” Verify Services Are Running

Check service health:

```powershell
# Check OCR
curl http://localhost:8000/health

# Check RAG  
curl http://localhost:8001/health
```

Expected response: `{"status":"healthy"}`

---

## ğŸ“Š Architecture Overview

```
User Browser (localhost:8501)
    â†“
Streamlit App (venv_main)
    â†“
    â”œâ”€â†’ OCR Service (venv_ocr, port 8000)
    â”‚   â””â”€â†’ Extracts text from files
    â”‚
    â””â”€â†’ RAG Service (venv_rag, port 8001)
        â”œâ”€â†’ Stores text in vector database
        â””â”€â†’ Queries Ollama LLM (port 11434)
```

---

## ğŸ› Troubleshooting

### Port Already in Use

```powershell
# Find process using port
netstat -ano | findstr :8000

# Kill process
taskkill /PID <PID> /F
```

### Ollama Not Running

```powershell
# Start Ollama
ollama serve

# Pull model (in another terminal)
ollama pull qwen3:1.7b
```

### Service Won't Start

- Make sure you're in the project root directory
- Check that the correct virtual environment is activated
- Look for error messages in the terminal

### PowerShell Execution Policy Error

If you see "running scripts is disabled", the dependencies are still installed correctly (they install to the global Python, not the venv). The services will still work!

---

## ğŸ“ Notes

- **Dependency Conflicts Solved**: Each service runs in its own isolated environment
- **No More Import Errors**: Services communicate via HTTP APIs, not direct imports
- **Lightweight Main App**: Streamlit app only needs 2 packages!
- **Easy Scaling**: Each service can be scaled independently

---

## ğŸ¯ Next Steps

1. **Test the pipeline**: Upload a document and ask questions
2. **Read the docs**: Check `pipeline/MICROSERVICES_GUIDE.md` for details
3. **Customize**: Modify services in `pipeline/` folder as needed

---

## ğŸ“š Additional Resources

- **Main Guide**: `START_HERE.md`
- **Microservices Guide**: `pipeline/MICROSERVICES_GUIDE.md`
- **Pipeline README**: `pipeline/README.md`
- **RAG README**: `RAG/README.md`

---

## ğŸ‰ You're All Set!

Everything is installed and ready to go. Just run `.\start_all_services.bat` and start processing documents!

Enjoy your OCR-RAG pipeline! ğŸš€

